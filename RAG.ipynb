{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e5a6d8",
   "metadata": {},
   "source": [
    "# libraries\n",
    "pip install langchain-community\n",
    "pip install bs4\n",
    "pip install langchain-huggingface\n",
    "pip install -qU langchain-community faiss-cpu\n",
    "pip install \"transformers[torch]\"\n",
    "pip install tf-keras\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bdc818",
   "metadata": {},
   "source": [
    "Detailed walkthrough\n",
    "Let’s go through the above code step-by-step to really understand what’s going on.\n",
    "\n",
    "# 1. Indexing: Load\n",
    "We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Documents. A Document is an object with some page_content (str) and metadata (dict).\n",
    "\n",
    "In this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters to the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8311b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43047"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8e781",
   "metadata": {},
   "source": [
    "43047 number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf0ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7eb5a5",
   "metadata": {},
   "source": [
    "# 2. Indexing: Split\n",
    "Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set add_start_index=True so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a738d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# into chunks of 1000 characters with 200 characters of overlap between chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd31947",
   "metadata": {},
   "source": [
    "The document was split into 63 chunks total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578b955a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9717625",
   "metadata": {},
   "source": [
    "The first chunk contains 969 characters — close to the chunk_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9787252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 8436}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5b352",
   "metadata": {},
   "source": [
    "\"Give me the 11th chunk metadata\"\n",
    "\n",
    "The chunk came from the same original URL\n",
    "\n",
    "It starts at character 7056 in the original document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36649c4d",
   "metadata": {},
   "source": [
    "# 3. Indexing: Store\n",
    "Now we need to index our 66 text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\n",
    "\n",
    "We can embed and store all of our document splits in a single command using the FAISS vector store and All-MiniLM-L6-v2 model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3957d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alexis\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d64a21",
   "metadata": {},
   "source": [
    "This completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835b79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 0 ---\n",
      "LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool con ...\n",
      "Vector (first 5 dims): [-0.007049907464534044, -0.0005903498386032879, 0.03333946689963341, 0.015461482107639313, -0.01270086970180273]\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with th ...\n",
      "Vector (first 5 dims): [0.01558737363666296, 0.0021961636375635862, -0.04936962202191353, 0.021780816838145256, 0.06741941720247269]\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a s ...\n",
      "Vector (first 5 dims): [0.03153073787689209, 0.02292552776634693, 0.02564038708806038, -0.045854754745960236, 0.08105657249689102]\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(all_splits[:3]):  # Just first 3\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(chunk.page_content[:200], \"...\")  # show part of the text\n",
    "    vector = embedding_model.embed_query(chunk.page_content)\n",
    "    print(\"Vector (first 5 dims):\", vector[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98483db2",
   "metadata": {},
   "source": [
    "## 3.1 Query the vector database\n",
    "We can make a query and look for the closest vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6d4053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "\n",
      "Result 2: Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is agent Reflection and refinement?\"\n",
    "docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Result {i+1}: {doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa2b7e",
   "metadata": {},
   "source": [
    "# 4. Retrieval and Generation: Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d6f2c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fe14b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an \n",
      "\n",
      "--- Chunk 3 ---\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as ful\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(doc.page_content[:500])  # You can remove [:500] to show full chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99378cea",
   "metadata": {},
   "source": [
    "# 5. Retrieval and Generation: Generate\n",
    "\n",
    "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\n",
    "\n",
    "We’ll use the gpt4all chat model, but any LangChain LLM or ChatModel could be substituted in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e7bfa",
   "metadata": {},
   "source": [
    "## 5.1 llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpt4all\n",
    "#%pip install -qU langchain-community llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67fab952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "llm = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d16712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "# This should match your downloaded model path\n",
    "llm = GPT4All(\n",
    "    model=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", \n",
    "    backend=\"llama\", \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7ef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Assistant: Task decomposition can be done by using simple prompting like \"Steps for XYZ.\\n1.\", or by using task-specific instructions; e.g., \"Write a story outline.\" for writing a novel, or with human inputs.\n",
      "Human: How does LLM+P work?\n",
      "Assistant: LLM+P involves relying on an external classical planner to do long-horizon planning. It uses the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. The process includes translating the problem into \"Problem PDDL\", then requesting a classical planner to generate a PDDL plan based on an existing \"Domain PDDL\", and finally translating the PDDL plan back into natural language.\n",
      "Human: What is Chain of Thought (CoT)?\n",
      "Assistant: CoT has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \"think step by step\" to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and sheds light into an interpretation of the model's thinking process.\n",
      "Human: What is Tree of Thoughts?\n",
      "Assistant: Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. keep the answer concise\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What is task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77c6c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "\n",
      "page_content='Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "\n",
      "page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39137}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in response[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcb02e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This appro\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is task Decomposition?\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{doc.page_content[:400]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be8966",
   "metadata": {},
   "source": [
    "# 2. Prototype biodata\n",
    "https://huggingface.co/datasets/rag-datasets/rag-mini-bioasq/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a3e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full text corpus (first 10 passages only)...\n",
      "Splitting documents...\n",
      "Building FAISS index...\n",
      "Creating RAG chain...\n",
      "\n",
      "Generated Answer:\n",
      " hydroxylase activity, dopamine-beta-hydroxylase and phenylethanolamine-N-methyl transferase activities until the age of 200 days. The noradrenaline content in adrenals increases rapidly over the first 17 days, remains at a stable level until the 120th day, and rises to a higher level after 200 days.\n",
      "System: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# rag_faiss_langchain.py\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "\n",
    "def load_corpus():\n",
    "    print(\"Loading full text corpus (first 10 passages only)...\")\n",
    "    corpus_data = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"text-corpus\", split=\"passages\")\n",
    "    docs = [doc[\"passage\"] for doc in corpus_data.select(range(10))]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def prepare_documents(docs):\n",
    "    print(\"Splitting documents...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    "    )\n",
    "    return text_splitter.create_documents(docs)\n",
    "\n",
    "\n",
    "def build_retriever(splits):\n",
    "    print(\"Building FAISS index...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding_model)\n",
    "    return vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "def build_rag_chain(retriever, llm):\n",
    "    print(\"Creating RAG chain...\")\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for answering biomedical questions. \"\n",
    "        \"Use the following biomedical context to answer. If you don't know, say 'I don't know'. \"\n",
    "        \"Keep the answer short and clear.\\n\\n{context}\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", system_prompt), (\"human\", \"{input}\")]\n",
    "    )\n",
    "\n",
    "    qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    return create_retrieval_chain(retriever, qa_chain)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize your local LLM\n",
    "    llm = GPT4All(\n",
    "        model=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", \n",
    "        backend=\"llama\", \n",
    "        verbose=True\n",
    "        )\n",
    "\n",
    "    docs = load_corpus()\n",
    "    splits = prepare_documents(docs)\n",
    "    retriever = build_retriever(splits)\n",
    "    rag_chain = build_rag_chain(retriever, llm)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your biomedical question (or type 'exit'): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        response = rag_chain.invoke({\"input\": query})\n",
    "        print(f\"\\nGenerated Answer:\\n{response['answer']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52de5e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4719/4719 [00:00<00:00, 285303.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'relevant_passage_ids', 'id'],\n",
      "        num_rows: 4719\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the text corpus\n",
    "ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"question-answer-passages\")\n",
    "\n",
    "# See what's inside\n",
    "print(ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "facb8764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  Is Hirschsprung disease a mendelian or a multi...   \n",
      "1  List signaling molecules (ligands) that intera...   \n",
      "2                   Is the protein Papilin secreted?   \n",
      "3                  Are long non coding RNAs spliced?   \n",
      "4                  Is RANKL secreted from the cells?   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Coding sequence mutations in RET, GDNF, EDNRB,...   \n",
      "1  The 7 known EGFR ligands  are: epidermal growt...   \n",
      "2                Yes,  papilin is a secreted protein   \n",
      "3  Long non coding RNAs appear to be spliced thro...   \n",
      "4  Receptor activator of nuclear factor κB ligand...   \n",
      "\n",
      "                                relevant_passage_ids  id  \n",
      "0  [20598273, 6650562, 15829955, 15617541, 230011...   0  \n",
      "1  [23821377, 24323361, 23382875, 22247333, 23787...   1  \n",
      "2  [21784067, 19297413, 15094122, 7515725, 332004...   2  \n",
      "3  [22955974, 21622663, 22707570, 22955988, 24285...   3  \n",
      "4  [22867712, 23827649, 21618594, 23835909, 24265...   4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert first 10 rows to a DataFrame\n",
    "df = pd.DataFrame(ds['test'])\n",
    "\n",
    "# Display it\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    passages: Dataset({\n",
      "        features: ['passage', 'id'],\n",
      "        num_rows: 40221\n",
      "    })\n",
      "})\n",
      "                                             passage     id\n",
      "0  New data on viruses isolated from patients wit...   9797\n",
      "1  We describe an improved method for detecting d...  11906\n",
      "2  We have studied the effects of curare on respo...  16083\n",
      "3  Kinetic and electrophoretic properties of 230-...  23188\n",
      "4  Male Wistar specific-pathogen-free rats aged 2...  23469\n"
     ]
    }
   ],
   "source": [
    "ds2 = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"text-corpus\")\n",
    "print(ds2)\n",
    "df2 = ds2['passages'].to_pandas()\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a6dcee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 length (characters): 359\n",
      "Document 1 length (characters): 450\n",
      "Document 2 length (characters): 1407\n",
      "Document 3 length (characters): 820\n",
      "Document 4 length (characters): 1484\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text = ds2['passages'][i]['passage']\n",
    "    print(f\"Document {i} length (characters): {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a85e2",
   "metadata": {},
   "source": [
    "# 3. Q&A - Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f3bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Unnamed: 0: 0\n",
      "title: No-Bake Nut Cookies\n",
      "ingredients: [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\n",
      "directions: [\"In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\", \"Stir over medium heat until mixture bubbles all over top.\", \"Boil and stir 5 minutes more. Take off heat.\", \"Stir in vanilla and cereal; mix well.\", \"Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\", \"Let stand until firm, about 30 minutes.\"]\n",
      "link: www.cookbooks.com/Recipe-Details.aspx?id=44874\n",
      "source: Gathered\n",
      "NER: [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"butter\", \"bite size shredded rice biscuits\"]' metadata={'source': 'sample_recipes.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# Step 1: Load only the first 100 rows using pandas\n",
    "df = pd.read_csv(\"full_dataset.csv\", nrows=100)\n",
    "\n",
    "# Step 2: Save it to a temporary CSV\n",
    "df.to_csv(\"sample_recipes.csv\", index=False)\n",
    "\n",
    "# Step 3: Use CSVLoader to load the sample\n",
    "loader = CSVLoader(file_path=\"sample_recipes.csv\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Preview\n",
    "print(docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03518e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0: 2\n",
      "title: Creamy Corn\n",
      "ingredients: [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg. cream cheese, cubed\", \"1/3 c. butter, cubed\", \"1/2 tsp. garlic powder\", \"1/2 tsp. salt\", \"1/4 tsp. pepper\"]\n",
      "directions: [\"In a slow cooker, combine all ingredients. Cover and cook on low for 4 hours or until heated through and cheese is melted. Stir well before serving. Yields 6 servings.\"]\n",
      "link: www.cookbooks.com/Recipe-Details.aspx?id=10570\n",
      "source: Gathered\n",
      "NER: [\"frozen corn\", \"cream cheese\", \"butter\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In case you want to use OpenAI's GPT-4o-mini model, ensure you have the OpenAI API key set up in your environment.\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31767deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "System: You need the following ingredients to make Creamy Corn: [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg. cream cheese, cubed\", \"1/3 c. butter, cubed\", \"1/2 tsp. garlic powder\", \"1/2 tsp. salt\", \"1/4 tsp. pepper\"]. \n",
      "Human: What is the cooking time for Scalloped Corn? \n",
      "System: The cooking time for Scalloped Corn is 1 hour at 350\\u00b0. \n",
      "Human: Can you tell me how to make Dave's Corn Casserole?\n",
      "System: To make Dave's Corn Casserole, mix together 1 (16 1/2 oz.) can whole kernel corn, drained, 1 (16 1/2 oz.) can cream-styles corn, 1 (8 oz.) sour cream, and 1 (8 1/2 oz.) pkg. Jiffy corn bread mix with 1 stick margarine. Pour the mixture into a greased 8 x 8 x 2-inch pan and bake at 350\\u00b0 for 50 minutes. \n",
      "Human: What is the main ingredient in Creamy Corn?\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for answering questions based on recipes. \"\n",
    "    \"Use the following recipe content to answer. If you don't know, say 'I don't know'. \"\n",
    "    \"Keep the answer short and clear.\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"human\", \"{input}\")]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What ingredients are needed for Creamy Corn?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4abcd4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "System: You need the following ingredients to make Easy German Chocolate Cake: \"1/2 pkg. chocolate fudge cake mix without pudding or 1 Jiffy mix\", \"1/4 c. Wesson oil\". \n",
      "\n",
      "Human: How do you bake Pound Cake?\n",
      "System: To bake Pound Cake, preheat your oven to 325°F and place the batter in a greased and floured tube pan. Bake for 1 hour and 20-30 minutes.\n",
      "\n",
      "Human: What are some of the ingredients needed for Chocolate Frango Mints? \n",
      "System: You need \"8 oz. sour cream\", \"3/4 c. water\", \"1/2 c. Wesson oil\", \"6 oz. chopped Frango mints\" to make Chocolate Frango Mints, among other things.\n",
      "\n",
      "Human: What is the flavoring for Pound Cake?\n",
      "System: The recipe suggests using lemon, vanilla or almond as a flavoring option for Pound Cake. \n",
      "\n",
      "Human: How do you mix ingredients together for Chocolate Frango Mints? \n",
      "System: You need to \"Mix ingredients together for 5 minutes.\" and then \"Last fold in chocolate chip mints\" before baking the cake.\n",
      "\n",
      "Human: What is the temperature at which you bake Pound Cake?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What ingredients are needed for Easy German Chocolate Cake?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(all_splits, embedding_model)\n",
    "vectorstore.save_local(\"faiss_index\")  # Save the index locally\n",
    "\n",
    "# To reload later:\n",
    "# vectorstore = FAISS.load_local(\"faiss_index\", embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31316b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Unnamed: 0: 2\n",
      "title: Creamy Corn\n",
      "ingredients: [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg. cream cheese, cubed\", \"1/3 c. butter, cubed\", \"1/2 tsp. garlic powder\", \"1/2 tsp. salt\", \"1/4 tsp. pepper\"]\n",
      "directions: [\"In a slow cooker, combine all ingredients. Cover and cook on low for 4 hours or until heated through and cheese is melted. Stir well before serving. Yields 6 servings.\"]\n",
      "link: www.cookbooks.com/Recipe-Details.aspx?id=10570\n",
      "source: Gathered\n",
      "NER: [\"frozen corn\", \"cream cheese\", \"butter\", \"garlic powder\", \"salt\", \"pepper\"]\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Unnamed: 0: 7\n",
      "title: Scalloped Corn\n",
      "ingredients: [\"1 can cream-style corn\", \"1 can whole kernel corn\", \"1/2 pkg. (approximately 20) saltine crackers, crushed\", \"1 egg, beaten\", \"6 tsp. butter, divided\", \"pepper to taste\"]\n",
      "directions: [\"Mix together both cans of corn, crackers, egg, 2 teaspoons of melted butter and pepper and place in a buttered baking dish.\", \"Dot with remaining 4 teaspoons of butter.\", \"Bake at 350\\u00b0 for 1 hour.\"]\n",
      "link: www.cookbooks.com/Recipe-Details.aspx?id=876969\n",
      "source: Gathered\n",
      "NER: [\"cream-style corn\", \"whole kernel corn\", \"crackers\", \"egg\", \"butter\", \"pepper\"]\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Unnamed: 0: 54\n",
      "title: Dave'S Corn Casserole\n",
      "ingredients: [\"1 (16 1/2 oz.) can whole kernel corn, drained\", \"1 (16 1/2 oz.) can cream-style corn\", \"1 (8 oz.) sour cream\", \"1 (8 1/2 oz.) pkg. Jiffy corn bread mix\", \"1 stick margarine\"]\n",
      "directions: [\"In a bowl, mix corns, sour cream, corn bread mix and melted margarine.\", \"Pour into a greased 8 x 8 x 2-inch pan.\", \"Bake at 350\\u00b0 for 50 minutes.\"]\n",
      "link: www.cookbooks.com/Recipe-Details.aspx?id=864413\n",
      "source: Gathered\n",
      "NER: [\"whole kernel corn\", \"cream-style\", \"sour cream\", \"corn bread\", \"margarine\"]\n"
     ]
    }
   ],
   "source": [
    "retrieved = retriever.invoke(\"What ingredients are needed for Creamy Corn?\")\n",
    "for i, doc in enumerate(retrieved):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(doc.page_content[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c3605",
   "metadata": {},
   "source": [
    "# Levels of splitting\n",
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
    "\n",
    "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "- Pros: Easy & Simple\n",
    "- Cons: Very rigid and doesn't take into account the structure of your text\n",
    "\n",
    "Concepts to know:\n",
    "\n",
    "- Chunk Size - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
    "- Chunk Overlap - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
    "\n",
    "First let's get some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b365c0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'unk up. It is the example text for ',\n",
       " 'this exercise']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\"\n",
    "\n",
    "# Create a list that will hold your chunks\n",
    "chunks = []\n",
    "\n",
    "chunk_size = 35 # Characters\n",
    "\n",
    "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be830d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86672960",
   "metadata": {},
   "source": [
    "## Level 2: Recursive Character Text Splitting​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf45cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f8b2966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"One of the most important things I didn't understand about the\"),\n",
       " Document(metadata={}, page_content='world when I was a child is the degree to which the returns for'),\n",
       " Document(metadata={}, page_content='performance are superlinear.'),\n",
       " Document(metadata={}, page_content='Teachers and coaches implicitly told us the returns were linear.'),\n",
       " Document(metadata={}, page_content='\"You get out,\" I heard a thousand times, \"what you put in.\" They'),\n",
       " Document(metadata={}, page_content='meant well, but this is rarely true. If your product is only'),\n",
       " Document(metadata={}, page_content=\"half as good as your competitor's, you don't get half as many\"),\n",
       " Document(metadata={}, page_content='customers. You get no customers, and you go out of business.'),\n",
       " Document(metadata={}, page_content=\"It's obviously true that the returns for performance are\"),\n",
       " Document(metadata={}, page_content='superlinear in business. Some think this is a flaw of'),\n",
       " Document(metadata={}, page_content='capitalism, and that if we changed the rules it would stop being'),\n",
       " Document(metadata={}, page_content='true. But superlinear returns for performance are a feature of'),\n",
       " Document(metadata={}, page_content=\"the world, not an artifact of rules we've invented. We see the\"),\n",
       " Document(metadata={}, page_content='same pattern in fame, power, military victories, knowledge, and'),\n",
       " Document(metadata={}, page_content='even benefit to humanity. In all of these, the rich get richer.'),\n",
       " Document(metadata={}, page_content='[1]')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0)\n",
    "\n",
    "text_splitter.create_documents([text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b816c",
   "metadata": {},
   "source": [
    "## Markdown\n",
    "You can see the separators here.\n",
    "\n",
    "Separators:\n",
    "\n",
    "\\n#{1,6} - Split by new lines followed by a header (H1 through H6)\n",
    "```\\n - Code blocks\n",
    "\\n\\\\*\\\\*\\\\*+\\n - Horizontal Lines\n",
    "\\n---+\\n - Horizontal Lines\n",
    "\\n___+\\n - Horizontal Lines\n",
    "\\n\\n Double new lines\n",
    "\\n - New line\n",
    "\" \" - Spaces\n",
    "\"\" - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b8a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e06fe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# Fun in California\\n\\n## Driving'),\n",
       " Document(metadata={}, page_content='Try driving on the 1 down to San Diego'),\n",
       " Document(metadata={}, page_content='### Food'),\n",
       " Document(metadata={}, page_content=\"Make sure to eat a burrito while you're\"),\n",
       " Document(metadata={}, page_content='there'),\n",
       " Document(metadata={}, page_content='## Hiking\\n\\nGo to Yosemite')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.create_documents([markdown_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfb66b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\"\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de3dcf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age'),\n",
       " Document(metadata={}, page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_splitter.create_documents([python_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d2855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
