{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e5a6d8",
   "metadata": {},
   "source": [
    "# libraries\n",
    "pip install langchain-community\n",
    "pip install bs4\n",
    "pip install langchain-huggingface\n",
    "pip install -qU langchain-community faiss-cpu\n",
    "pip install \"transformers[torch]\"\n",
    "pip install tf-keras\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bdc818",
   "metadata": {},
   "source": [
    "Detailed walkthrough\n",
    "Let’s go through the above code step-by-step to really understand what’s going on.\n",
    "\n",
    "# 1. Indexing: Load\n",
    "We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Documents. A Document is an object with some page_content (str) and metadata (dict).\n",
    "\n",
    "In this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters to the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8311b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43130"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8e781",
   "metadata": {},
   "source": [
    "43130 number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7eb5a5",
   "metadata": {},
   "source": [
    "# 2. Indexing: Split\n",
    "Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set add_start_index=True so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a738d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# into chunks of 1000 characters with 200 characters of overlap between chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd31947",
   "metadata": {},
   "source": [
    "The document was split into 66 chunks total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578b955a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9717625",
   "metadata": {},
   "source": [
    "The first chunk contains 969 characters — close to the chunk_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9787252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7055}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5b352",
   "metadata": {},
   "source": [
    "\"Give me the 11th chunk metadata\"\n",
    "\n",
    "The chunk came from the same original URL\n",
    "\n",
    "It starts at character 7056 in the original document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36649c4d",
   "metadata": {},
   "source": [
    "# 3. Indexing: Store\n",
    "Now we need to index our 66 text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\n",
    "\n",
    "We can embed and store all of our document splits in a single command using the FAISS vector store and All-MiniLM-L6-v2 model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3957d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alexis\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d64a21",
   "metadata": {},
   "source": [
    "This completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "835b79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 0 ---\n",
      "LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool con ...\n",
      "Vector (first 5 dims): [-0.007049907464534044, -0.0005903498386032879, 0.03333946689963341, 0.015461482107639313, -0.01270086970180273]\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with th ...\n",
      "Vector (first 5 dims): [0.0036475660745054483, 0.011537445709109306, -0.055276770144701004, 0.05251268297433853, 0.07744487375020981]\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposit ...\n",
      "Vector (first 5 dims): [0.03652041405439377, 0.03538128361105919, 0.013849335722625256, -0.031334374099969864, 0.059988126158714294]\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(all_splits[:3]):  # Just first 3\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(chunk.page_content[:200], \"...\")  # show part of the text\n",
    "    vector = embedding_model.embed_query(chunk.page_content)\n",
    "    print(\"Vector (first 5 dims):\", vector[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98483db2",
   "metadata": {},
   "source": [
    "## 3.1 Query the vector database\n",
    "We can make a query and look for the closest vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a6d4053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "Result 2: Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is agent Reflection and refinemen?\"\n",
    "docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Result {i+1}: {doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa2b7e",
   "metadata": {},
   "source": [
    "# 4. Retrieval and Generation: Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d6f2c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e88c118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fe14b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big ta\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the sub\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as ful\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(doc.page_content[:500])  # You can remove [:500] to show full chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99378cea",
   "metadata": {},
   "source": [
    "# 5. Retrieval and Generation: Generate\n",
    "\n",
    "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\n",
    "\n",
    "We’ll use the gpt4all chat model, but any LangChain LLM or ChatModel could be substituted in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e7bfa",
   "metadata": {},
   "source": [
    "## 5.1 llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpt4all\n",
    "#%pip install -qU langchain-community llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fab952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "llm = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d16712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "# This should match your downloaded model path\n",
    "llm = GPT4All(\n",
    "    model=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", \n",
    "    backend=\"llama\", \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c179fe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nTask Decomposition refers to breaking down complex tasks into smaller, manageable steps or subgoals that can be planned and executed individually. This process helps agents like LLM-powered autonomous systems plan ahead and make decisions more effectively.\\nThanks for asking! '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77c6c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}\n",
      "\n",
      "page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}\n",
      "\n",
      "page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39220}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in response[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcb02e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task dec\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is task Decomposition?\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{doc.page_content[:400]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a85e2",
   "metadata": {},
   "source": [
    "# 2. Example with a document text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76ff8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"example.txt\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3187e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)\n",
    "chunks = splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31767deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "mbedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=mbedding_model)\n",
    "chunk_vectors = mbedding_model.embed_documents([chunk.page_content for chunk in chunks])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da38a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.032221630215644836, 0.006504561752080917, 0.0596463568508625, 0.07347435504198074, 0.010449625551700592]\n"
     ]
    }
   ],
   "source": [
    "print(chunk_vectors[0][:5])  # preview first 5 dimensions of first vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62a0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(chunks, mbedding_model)\n",
    "db.save_local(\"faiss_index\")  # Save the index locally\n",
    "\n",
    "# To reload later:\n",
    "# db = FAISS.load_local(\"faiss_index\", embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fabd3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: Title: Climate Change in Latin America\n",
      "\n",
      "Result 2: Climate change is increasingly affecting Latin Americaâ€™s biodiversity, water resources, and agricultural productivity. Rising temperatures have led\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is climate\"\n",
    "docs = db.similarity_search(query, k=2)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Result {i+1}: {doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31316b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
